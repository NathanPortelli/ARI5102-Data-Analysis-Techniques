{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1fd7e54",
   "metadata": {},
   "source": [
    "### Importing Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "861523d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For Task 3\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.ensemble~ import RandomForestClassifier\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73806599",
   "metadata": {},
   "source": [
    "### Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf8f40fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing and loading the dataset\n",
    "df = pd.read_csv(\"dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afd8900",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3706530a",
   "metadata": {},
   "source": [
    "#### To get a feel for the employed dataset, do some initial exploration and answer the following questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a330ed",
   "metadata": {},
   "source": [
    "### 1.1: How many questions exist in the survey?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dbbdec-917f-4f3d-a9e0-833d3fdcd219",
   "metadata": {},
   "source": [
    "In the dataset's header, answers of the same question share the same 'prefix'. For example, `q3_1`, `q3_2`, ... `q3_10` are all possible answers of the same question. Therefore, we can remove the content after the underscore from the header, remaining with `q1`, `q2`, ... `q10`, and only count the unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8b65044",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions in the survey: 10\n"
     ]
    }
   ],
   "source": [
    "# List of columns, each of which represent the options for each question\n",
    "columns = df.columns\n",
    "\n",
    "# Number of unique question prefixes\n",
    "uniq_questions = set([col.split('_')[0] for col in columns])\n",
    "\n",
    "# Total number of actual questions\n",
    "num_questions = len(uniq_questions)\n",
    "print(\"Number of questions in the survey:\", num_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff81067",
   "metadata": {},
   "source": [
    "### 1.2: How many respondents are in the survey?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dd30d8-3380-4175-b459-e722a101f34b",
   "metadata": {},
   "source": [
    "Each row within the dataset represents a single unique respondent with their unique responses, apart from the first row which serves as a header which contains details on the content on each column. Therefore, counting the number of available rows in the dataset bar the first row gives us the number of respondents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07289b16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of respondents in the survey: 2288\n"
     ]
    }
   ],
   "source": [
    "# Returns the number of rows without header, which represents the number of respondents\n",
    "num_respondents = df.shape[0]\n",
    "\n",
    "print(\"Number of respondents in the survey:\", num_respondents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14471690",
   "metadata": {},
   "source": [
    "### 1.3: What are the different question types based on their selection options?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074dc1a3-5846-44dd-bda5-ab7c4ad2227a",
   "metadata": {},
   "source": [
    "The possible question types based on the respondents' selections are divided into two, whether the question permits for:\n",
    "1. Single or multiple responses\n",
    "2. Single or multiple options\n",
    "\n",
    "To determine the single/multiple selection type for each question, the code iterates through the dataset columns, splitting each column name to extract the question prefix. It then checks whether the prefix already exists in the selection_types dictionary. If it does, it evaluates whether the selection type is already marked as `Multiple`; if not, it updates the selection type to `Multiple`. If the question prefix is not found in the dictionary, it's assumed to be a single-selection question, and it's marked accordingly.\n",
    "\n",
    "Next, the code determines the question types by iterating through the dataset columns again. For each column, it extracts the question prefix and checks if it exists in the question_types dictionary. If not, it creates an entry for the question prefix. Then, it calls the determine_question_type function to add the question type to the corresponding question prefix entry.\n",
    "\n",
    "<!-- First, we ascertain whether each question allows for a single or multiple responses. This is done by iterating over the dataset columns, calculating the sum of responses for each respondent across related columns. Based on the maximum sum, we categorise the question as either \"Single\" or \"Multiple\" responses.\n",
    "\n",
    "We then identify if each question permits single or multiple options. We traverse the dataset columns, updating a dictionary with the selection type for each question prefix. If multiple selections are detected, the dictionary entry is updated to \"Multiple\"; otherwise, it defaults to \"Single\". -->\n",
    "\n",
    "This information is then combined for each question. The result for \"responses\" is omitted when the question only allows for a single option, as it is not possible for a question with one input to be able to have multiple response options inputted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c6fa533",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question Types:\n",
      "q1: Single options\n",
      "q2: Single options\n",
      "q3: Multiple options and Single responses\n",
      "q4: Multiple options and Single responses\n",
      "q5: Multiple options and Single responses\n",
      "q6: Multiple options and Multiple responses\n",
      "q7: Multiple options and Multiple responses\n",
      "q8: Multiple options and Multiple responses\n",
      "q9: Multiple options and Multiple responses\n",
      "q10: Multiple options and Multiple responses\n"
     ]
    }
   ],
   "source": [
    "## todo: optimise!!!\n",
    "\n",
    "# Function to determine question type\n",
    "def determine_question_type(question):\n",
    "    # Get columns related to the question\n",
    "    question_columns = [col for col in df.columns if col.startswith(question)]\n",
    "    \n",
    "    # Check if it's a single response or multiple responses question\n",
    "    responses_sum = df[question_columns].sum(axis=1)\n",
    "    max_sum = responses_sum.max()\n",
    "    if max_sum == 1:\n",
    "        return \"Single\"\n",
    "    else:\n",
    "        return \"Multiple\"\n",
    "    \n",
    "# Get question types for all questions\n",
    "question_types = {}\n",
    "\n",
    "# Determine single/multiple selection type for each question\n",
    "selection_types = {}\n",
    "\n",
    "for col in df.columns:\n",
    "    question_prefix = col.split('_')[0]\n",
    "    if question_prefix in selection_types:\n",
    "        # If the question prefix already exists, check if it's a multiple-selection question\n",
    "        if selection_types[question_prefix] != 'Multiple':\n",
    "            selection_types[question_prefix] = 'Multiple'\n",
    "    else:\n",
    "        # If the question prefix is not in the dictionary, assume it's a single-selection question\n",
    "        selection_types[question_prefix] = 'Single'\n",
    "\n",
    "for col in df.columns:\n",
    "    question_prefix = col.split('_')[0]\n",
    "    if question_prefix not in question_types:\n",
    "        question_types[question_prefix] = set()\n",
    "    \n",
    "    # Add the question type\n",
    "    question_types[question_prefix].add(determine_question_type(question_prefix))\n",
    "\n",
    "# Print combined question and selection types\n",
    "print(\"Question Types:\")\n",
    "for question, q_types in question_types.items():\n",
    "    q_type_str = \" and \".join(q_types)\n",
    "    print(f\"{question}: {selection_types[question]} options\", end = \"\")\n",
    "    if selection_types[question] == 'Multiple':\n",
    "        print(f\" and {q_type_str} responses\")\n",
    "    else:\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032eee36",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8788a5e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### If you were asked to build a model for a question in the dataset, using the rest of the questions as explanatory variables:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdabdc86",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.1: Which performance measures would you use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262376e6",
   "metadata": {},
   "source": [
    "When building a model for questions in the dataset, the choice of performance measures hinges upon the nature of the target variable and the specific objectives of the analysis. As shown in Task 1, the dataset comprises questions that fall under different classification categories, each demanding different types of evaluation metrics.\n",
    "\n",
    "For instance, questions 1 and 2 present Binary Classification Problems, with only two possible outcomes [1]. For these types of questions, measures such as Accuracy, Precision, Recall, F1-score, and the Area under the ROC curve (AUC-ROC) may be used to assess the performance of the model.\n",
    "\n",
    "On the other hand, questions 3, 4 and 5 can be described as Multi-Class Classification Problems with Mutually Exclusive Classes, as they entail multiple potential outputs, yet only one output can be selected [1]. In this case, similar performance measures as questions 1 and 2 could be used, being accuracy, precision, recall, F1-Score and confusion matrices. Additionally, the Micro F1-score, which aggregates F1-scores across all classes using micro-averaging, provides a balanced assessment of the model's performance, especially when dealing with class imbalance [2]. Apart from making use of Accuracy, which provides an overall measure of how correct the model's predictions are, Balanced Accuracy can also be used, which gives equal weight to each class and is therefore insensitive to class distribution [2]. This could therefore be useful if the survey is found to not have a balanced distribution of responses.\n",
    "\n",
    "Finally, questions 6 until 10 pose Multi-Class Classification Problems with Non-Exclusive Classes, with the difference from the previous set of questions being that multiple outputs can be simultaneously selected [1]. Here, evaluation metrics such as Hamming Loss, precision at k, recall at k, F1-score at k, and subset accuracy become instrumental in gauging the model's predictive capabilities.\n",
    "\n",
    "##### todo: add about unbalance - question 1 is unbalanced\n",
    "##### Todo: add references"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7829ed0c",
   "metadata": {},
   "source": [
    "### 2.2: Does the selection of the performance metric depend on the type of question?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62292ee",
   "metadata": {},
   "source": [
    "Yes, as shown in Task 2 Part 1, the selection of the performance metric depends on the type of question being addressed. Different types of questions present distinct characteristics, necessitating the use of specific performance metrics that are most suitable for evaluating the model’s effectiveness. Here's how the choice of performance metric aligns with the type of question:\n",
    "\n",
    "Binary Classification (Question 1 and 2):\n",
    "- In these problems, where there are only two possible outcomes, metrics such as accuracy, precision, recall, F1-score, and the area under the ROC curve (AUC-ROC) are commonly used. These metrics provide insights into the model's ability to correctly classify instances into one of the two classes, taking into account True Positives (TP) represent correctly classified positive instances, True Negatives (TN) denote correctly classified negative instances, False Positives (FP) indicate negative instances incorrectly classified as positive, and False Negatives (FN) signify positive instances incorrectly classified as negative [1].\n",
    "\n",
    "Multi-Class Classification (Question 3 to 5):\n",
    "- In the case of Multi-Class Classification Problems, where there are multiple possible outputs but only one can be selected, accuracy, precision, recall, and F1-score can also be utilised, and also confusion matrices. These metrics enable the assessment of the model's performance across multiple classes, providing insights into its ability to distinguish between different categories.\n",
    "\n",
    "Multi-Label Classification (Question 6 to 10):\n",
    "- For multi-label classification problems, where multiple outputs can be simultaneously selected, performance metrics such as Hamming Loss, precision at k, recall at k, F1-score at k, and subset accuracy are often employed. These metrics are tailored to evaluate the model's ability to predict multiple labels for each instance accurately, considering the complexities of multi-label prediction tasks.\n",
    "\n",
    "Each performance metric has its strengths and weaknesses, and the choice depends on the specific characteristics of the dataset and the objectives of the modeling task. Therefore, it's essential to carefully consider the nature of the target variable and the context of the problem before selecting the appropriate performance metric for evaluation.\n",
    "\n",
    "##### todo: add references"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0712f76f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8da9bff",
   "metadata": {},
   "source": [
    "#### Consider the fifth question (q5) as a response variable and build a predictive model using some or all other questions as explanatory variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e2eff9",
   "metadata": {},
   "source": [
    "### 3.1.1: Implement a suitable algorithm for this task. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ed0140",
   "metadata": {},
   "source": [
    "##### From [1]:\n",
    "The size of your dataset will greatly influence the type of algorithm\n",
    "you can use. That said there is no golden rule to the amount of data required to train\n",
    "an ML model. However, some ML algorithms cope better where there is less\n",
    "available data. For example, an image classification task can require tens of thou\u0002sands of images. Regression tasks require many more observations than features\n",
    "(10\u0001 as much) while NLP can require 1000’s of examples due to a large number of\n",
    "words and complex phrases. Some algorithms such as neural networks can be\n",
    "extremely data-hungry while other algorithms such as a random forest can produce\n",
    "reasonably good results with limited amounts of data. The process of finding the\n",
    "correct model based on the size of available data is an empirical one\n",
    "\n",
    "##### todo: change\n",
    "\n",
    "We chose to use the Random Forest algorithm for several reasons:\n",
    "\n",
    "Ensemble Learning: Random Forest is an ensemble learning method that combines multiple decision trees to create a more robust and accurate model. It works by building multiple decision trees during training and outputting the mode of the classes (classification) or the mean prediction (regression) of the individual trees. This ensemble approach helps to reduce overfitting and increase the generalization of the model.\n",
    "\n",
    "Handles Non-linearity and Interactions: Random Forest can effectively capture non-linear relationships and interactions between variables in the dataset. This is particularly useful when dealing with complex data where the relationship between the features and the target variable is not straightforward.\n",
    "\n",
    "Feature Importance: Random Forest provides a measure of feature importance, which can be valuable for understanding the contribution of each feature to the predictive performance of the model. This can aid in feature selection and feature engineering, leading to better model performance.\n",
    "\n",
    "Robust to Overfitting: Random Forest is less prone to overfitting compared to individual decision trees, especially when the number of trees in the forest is sufficiently large. By aggregating the predictions from multiple trees, Random Forest reduces the variance of the model and improves its generalization ability.\n",
    "\n",
    "Works well with Mixed Data Types: Random Forest can handle both numerical and categorical features without requiring feature scaling or encoding. This makes it convenient for datasets with mixed data types, such as the survey dataset provided, where some questions may have categorical response options.\n",
    "\n",
    "Overall, Random Forest is a versatile and powerful algorithm that tends to perform well across a wide range of datasets and can be a suitable choice for building predictive models in many real-world scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd92721",
   "metadata": {},
   "source": [
    "The process:\n",
    "\n",
    "1. Extract Independent and Dependent Variables in Dataset\n",
    "2. Identify and handle Missing Values\n",
    "3. Encoding Categorical Variables\n",
    "4. Splitting Dataset\n",
    "5. Feature Scaling\n",
    "6. Data Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbf3fb0",
   "metadata": {},
   "source": [
    "#### Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bf43c08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extracting response variable (q5) and explanatory variables\n",
    "response_columns = df.filter(regex='^q5_').columns.tolist()\n",
    "explanatory_columns = df.drop(columns=response_columns)\n",
    "# y = df[response_columns]\n",
    "# X = explanatory_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba17722",
   "metadata": {},
   "source": [
    "#### Handling Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91ed35a-fbca-40d5-9a08-38f54ced8bbe",
   "metadata": {},
   "source": [
    "Next, let's divide the dataset and check which questions have missing responses, if any. After grouping the different questions, we count the number of values per question by isolating the data using the `isnull().sum()` function to calculate the number of missing responses. Since the missing values account for the entire response and not just a single possible response per question, we divide the total number of missing values per the amount of possible responses, hence giving the amount of missing responses per question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "329c766b-b0ce-46fe-8035-55242b99299e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Responses in Explanatory Variables:\n",
      "q1: 0\n",
      "q2: 0\n",
      "q3: 0\n",
      "q4: 0\n",
      "q6: 811\n",
      "q7: 0\n",
      "q8: 1255\n",
      "q9: 0\n",
      "q10: 0\n",
      "\n",
      "Missing Responses in Response Variable:\n",
      "q5: 0\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary to store sections\n",
    "sections = {}\n",
    "\n",
    "# Iterate through columns and group them by question number prefix\n",
    "for column in explanatory_columns.columns.tolist():\n",
    "    question_number = column.split('_')[0]  # Extract question number prefix\n",
    "    if question_number in sections:\n",
    "        sections[question_number].append(column)\n",
    "    else:\n",
    "        sections[question_number] = [column]\n",
    "\n",
    "# Iterate through sections to calculate missing values\n",
    "missing_values_per_question = {}\n",
    "for section, columns in sections.items():\n",
    "    section_data = explanatory_columns[columns]\n",
    "    missing_values_per_question[section] = section_data.isnull().sum()\n",
    "\n",
    "# Calculate average missing values per column for each question\n",
    "average_missing_values_per_question = {}\n",
    "for question, missing_values in missing_values_per_question.items():\n",
    "    num_columns = len(sections[question])\n",
    "    total_missing_values = missing_values.sum()\n",
    "    average_missing_values_per_question[question] = total_missing_values / num_columns\n",
    "\n",
    "# Print average missing values per column for each question\n",
    "print(\"Missing Responses in Explanatory Variables:\")\n",
    "for question, average_missing_values in average_missing_values_per_question.items():\n",
    "    print(f\"{question}: {int(average_missing_values)}\")\n",
    "    \n",
    "# Checking Missing Values in Response Variable\n",
    "missing_values_response = df[response_columns].isna().sum().sum()\n",
    "print(\"\\nMissing Responses in Response Variable:\")\n",
    "print(f\"q5: {missing_values_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86874590-8f9a-41cc-b971-93b54532fe44",
   "metadata": {},
   "source": [
    "Questions q6 and q8 exhibit a notable number of missing responses, with 811 and 1255 missing values respectively. Given that the total number of respondents in the dataset is 2288, this is quite a significant amount of missing responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2029ea42-dc20-4859-860b-f69be3479ce9",
   "metadata": {},
   "source": [
    "CHATGPT:\n",
    "\n",
    "\n",
    "Handling missing values in a dataset is a crucial step in data preprocessing before building a predictive model. Here are some common techniques to handle missing values:\n",
    "\n",
    "1. Removal: You can simply remove rows or columns with missing values. However, this approach might lead to loss of valuable data, especially if the missing values are not randomly distributed.\n",
    "2. Imputation: Replace missing values with a statistical measure such as mean, median, or mode of the column. This method maintains the structure of the dataset but may introduce bias, especially if the missing values are not missing at random.\n",
    "3. Predictive Imputation: Use predictive models to estimate missing values based on other available features. This method can provide more accurate imputations but requires more computational resources.\n",
    "4. K-nearest neighbors (KNN) imputation: Replace missing values with the average of nearest neighbors. This method is effective for datasets with non-linear patterns.\n",
    "5. Interpolation: Replace missing values by estimating values between existing data points. This method is useful for time-series data or datasets with a clear order.\n",
    "6. Multiple Imputation: Generate multiple imputations for missing values to capture uncertainty. This method is suitable for complex datasets with high variability.\n",
    "\n",
    "The choice of method depends on the nature of the data, the extent of missingness, and the assumptions about the missing data mechanism. It's often advisable to explore the data distribution and understand the reasons for missingness before selecting an appropriate imputation strategy. Additionally, it's good practice to evaluate the impact of imputation on the model performance.\n",
    "\n",
    "===========================\n",
    "\n",
    "Imputation with Mode: Since the survey dataset likely consists of categorical variables (responses to various questions), imputing missing values with the mode (most frequent response) of each respective question could be a suitable approach. This maintains the categorical nature of the data and doesn't introduce new values that might not accurately represent the respondents' choices.\n",
    "\n",
    "Create Separate Category for Missing Values: Alternatively, you could treat missing values as a separate category within each question. This approach allows the model to learn from the absence of a response as its own distinct category. However, this method may not be suitable if missingness is not informative or is too prevalent.\n",
    "\n",
    "Predictive Imputation: If missing values are not too prevalent and there are enough complete cases, you could use predictive imputation methods like K-nearest neighbors (KNN) or decision trees to estimate missing values based on the responses to other questions. However, this approach might be computationally expensive and may not be necessary unless there's a compelling reason to believe that missing values follow a specific pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2152e170-9ac2-43cd-a699-9a8cbfa1eab6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sklearn.impute import SimpleImputer\n",
    "\n",
    "# # Define the imputer with strategy as 'most_frequent' to impute with mode\n",
    "# imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# # Apply the imputer to the dataset\n",
    "# df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "\n",
    "# # Check if there are any missing values left\n",
    "# missing_values = df_imputed.isnull().sum().sum()\n",
    "# if missing_values == 0:\n",
    "#     print(\"No missing values left after imputation.\")\n",
    "# else:\n",
    "#     print(\"There are still missing values after imputation.\")\n",
    "\n",
    "# # Verify if there are any remaining missing values\n",
    "# print(\"Remaining missing values:\")\n",
    "# print(df_imputed.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42b08ecb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "q5_1     0\n",
       "q5_2     0\n",
       "q5_3     0\n",
       "q5_4     0\n",
       "q5_5     0\n",
       "q5_6     0\n",
       "q5_7     0\n",
       "q5_8     0\n",
       "q5_9     0\n",
       "q5_10    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking Missing Values in Response Variable\n",
    "y.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24908dc9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "q1_1      0\n",
       "q2_1      0\n",
       "q3_1      0\n",
       "q3_2      0\n",
       "q3_3      0\n",
       "         ..\n",
       "q10_6     0\n",
       "q10_7     0\n",
       "q10_8     0\n",
       "q10_9     0\n",
       "q10_10    0\n",
       "Length: 72, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking Amount of Missing Values in Explanatory Variable\n",
    "X.isna().sum() #*100/len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1412420-af63-4560-95f4-0dabe6e17ccf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values per Question:\n",
      "q1_1      0\n",
      "q2_1      0\n",
      "q3_1      0\n",
      "q3_2      0\n",
      "q3_3      0\n",
      "         ..\n",
      "q10_6     0\n",
      "q10_7     0\n",
      "q10_8     0\n",
      "q10_9     0\n",
      "q10_10    0\n",
      "Length: 82, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Check for missing values per question\n",
    "missing_values_per_question = df.isnull().sum()\n",
    "\n",
    "# Step 4: Display the missing values per question\n",
    "print(\"Missing Values per Question:\")\n",
    "print(missing_values_per_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00bbe85",
   "metadata": {},
   "source": [
    "### 3.1.2: Evaluate the implemented model using the metrics you proposed in Task 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0a67cb",
   "metadata": {},
   "source": [
    "### 3.1.3: Interpret the trained model (if possible) and the obtained results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fed8aa",
   "metadata": {},
   "source": [
    "#### Let’s assume that the options of the fifth question (q5) are semantically ordered, e.g. q5 corresponds to the question “How much do you like this photograph from 1-10?” with options q5_1=1, q5_2=2 and so on. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6df694",
   "metadata": {},
   "source": [
    "### 3.2.1: Implement a predictive model incorporating this new piece of information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce56c91",
   "metadata": {},
   "source": [
    "### 3.2.2: Select appropriate metrics to evaluate the performance of the model in this scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ab977c",
   "metadata": {},
   "source": [
    "## Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5679b88e",
   "metadata": {},
   "source": [
    "#### In this task, you should identify similar respondents. Specifically, you should do the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af31ce1",
   "metadata": {},
   "source": [
    "### 4.1: Create groups of “like-minded”/similar respondents by proposing and implementing one or more suitable algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fe9ae8",
   "metadata": {},
   "source": [
    "### 4.2: Evaluate whether the clusters you created are good enough."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b4489f",
   "metadata": {},
   "source": [
    "### 4.3: Compare different algorithms against each other to select one of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27e380e",
   "metadata": {},
   "source": [
    "### 4.4: Describe the groups you identified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b6a262",
   "metadata": {},
   "source": [
    "### 4.5: Create visualisations for the clustering results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908fbb61",
   "metadata": {},
   "source": [
    "### 4.6: How would you assign a new respondent to an existing group?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bb842b",
   "metadata": {},
   "source": [
    "## Task 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615c3a5a",
   "metadata": {},
   "source": [
    "#### Based on the results obtained from Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04da60c",
   "metadata": {},
   "source": [
    "### 5.1: Explain whether the clustering information could be used to build more accurate models for Task 3, and describe what you would do to build such models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7d2630-a82a-42c5-9cfa-39fd23763c56",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1288ca59-ee4d-4cbd-a47d-b72b5f9f1ee7",
   "metadata": {
    "tags": []
   },
   "source": [
    "[1] . Fergus and C. Chalmers, ‘Performance Evaluation Metrics’, in Applied Deep Learning: Tools, Techniques, and Implementation, P. Fergus and C. Chalmers, Eds., Cham: Springer International Publishing, 2022, pp. 115–138. doi: 10.1007/978-3-031-04420-5_5.\n",
    "[2] M. Grandini, E. Bagli, and G. Visani, ‘Metrics for Multi-Class Classification: an Overview’. arXiv, Aug. 13, 2020. doi: 10.48550/arXiv.2008.05756."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
